{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to CausalPilot","text":"<p>CausalPilot is a comprehensive framework for causal inference testing with multiple estimators.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multiple Estimators: T-Learner, S-Learner, X-Learner, DoubleML, Causal Forest.</li> <li>Dataset Loaders: Easy access to standard benchmarks (Twins, LaLonde, IHDP).</li> <li>Visualization: Tools for causal graphs and treatment effect heterogeneity.</li> <li>Type Safe: Fully typed codebase for robust development.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out the Introduction to learn more about the framework.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install causalpilot\n</code></pre>"},{"location":"api_documentation/","title":"CausalPilot API Documentation","text":""},{"location":"api_documentation/#overview","title":"Overview","text":"<p>CausalPilot provides a unified, scikit-learn-inspired API for causal inference. I designed it to be modular, type-safe, and easy to extend.</p>"},{"location":"api_documentation/#core-api-components","title":"Core API Components","text":""},{"location":"api_documentation/#1-causalmodel-class","title":"1. CausalModel Class","text":"<p>The <code>CausalModel</code> class is the main entry point. It handles data loading, graph definition, and effect estimation.</p>"},{"location":"api_documentation/#constructor","title":"Constructor","text":"<pre><code>from causalpilot.core import CausalModel\n\nmodel = CausalModel(data, treatment, outcome, graph=None)\n</code></pre> <p>Parameters: - <code>data: pd.DataFrame</code> - Observational dataset - <code>treatment: str</code> - Name of treatment variable - <code>outcome: str</code> - Name of outcome variable - <code>graph: CausalGraph, optional</code> - Causal graph structure</p>"},{"location":"api_documentation/#natural-language-interface-new","title":"Natural Language Interface (New!)","text":"<p>You can now initialize a model using plain English:</p> <pre><code>model = CausalModel.from_natural_language(\n    data=df,\n    query=\"I want to estimate the effect of 'treatment' on 'outcome', controlling for 'age' and 'income'.\"\n)\n</code></pre>"},{"location":"api_documentation/#methods","title":"Methods","text":"Method Parameters Returns Description <code>identify_effect()</code> None <code>List[str]</code> Identify adjustment set using backdoor criterion <code>estimate_effect(method, **kwargs)</code> <code>method: str, **kwargs</code> <code>Dict</code> Estimate causal effect using specified method"},{"location":"api_documentation/#2-inference-module","title":"2. Inference Module","text":"<p>I have implemented several state-of-the-art estimators. All estimators follow the <code>BaseEstimator</code> interface.</p>"},{"location":"api_documentation/#doubleml-estimator","title":"DoubleML Estimator","text":"<pre><code>from causalpilot.inference import DoubleML\n\nestimator = DoubleML(\n    ml_l=RandomForestRegressor,  # Outcome model\n    ml_m=RandomForestClassifier, # Treatment model  \n    n_folds=5                    # Cross-fitting folds\n)\nestimator.fit(X, T, Y)\neffect = estimator.estimate_effect()\n</code></pre>"},{"location":"api_documentation/#causal-forest-estimator","title":"Causal Forest Estimator","text":"<pre><code>from causalpilot.inference import CausalForest\n\nestimator = CausalForest(n_estimators=100)\nestimator.fit(X, T, Y)\ncate = estimator.predict(X_test)\n</code></pre>"},{"location":"api_documentation/#x-learner-new","title":"X-Learner (New!)","text":"<p>Best for unbalanced treatment groups. <pre><code>from causalpilot.inference import XLearner\n\nestimator = XLearner()\nestimator.fit(X, T, Y)\nate = estimator.estimate_effect()\n</code></pre></p>"},{"location":"api_documentation/#instrumental-variables-iv2sls-new","title":"Instrumental Variables (IV2SLS) (New!)","text":"<p>For unobserved confounding. <pre><code>from causalpilot.inference import IV2SLS\n\nestimator = IV2SLS()\nestimator.fit(X, T, Y, Z=instrument)\nate = estimator.estimate_effect()\n</code></pre></p>"},{"location":"api_documentation/#3-robustness-diagnostics-new","title":"3. Robustness &amp; Diagnostics (New!)","text":""},{"location":"api_documentation/#refutation","title":"Refutation","text":"<p>Validate your estimates by challenging assumptions.</p> <pre><code>from causalpilot.core import Refutation\n\nrefuter = Refutation(model)\n# Placebo Treatment: Effect should go to 0\nres = refuter.placebo_treatment_refutation()\n# Random Common Cause: Effect should not change\nres = refuter.random_common_cause_refutation()\n</code></pre>"},{"location":"api_documentation/#diagnostics","title":"Diagnostics","text":"<p>Check covariate balance.</p> <pre><code>from causalpilot.visualization.diagnostics import plot_covariate_balance\n\nplot_covariate_balance(data, treatment='T', covariates=['X1', 'X2'])\n</code></pre>"},{"location":"api_documentation/#validation-and-testing-api","title":"Validation and Testing API","text":""},{"location":"api_documentation/#input-validation","title":"Input Validation","text":"<pre><code>from causalpilot.utils import validate_data, validate_graph\n\n# Validate dataset for causal inference\nvalidate_data(data, treatment='T', outcome='Y')\n</code></pre>"},{"location":"api_documentation/#configuration","title":"Configuration","text":"<p>You can configure global settings like random state: <pre><code>import causalpilot.config as config\nconfig.RANDOM_STATE = 42\n</code></pre></p>"},{"location":"causal_inference_intro/","title":"Statistical Introduction to Causal Inference","text":""},{"location":"causal_inference_intro/#introduction","title":"Introduction","text":"<p>Causal inference represents one of the most fundamental challenges in statistics and data science: moving beyond correlation to establish true cause-and-effect relationships. In this document, I provide a comprehensive statistical introduction to causal inference, covering the theoretical foundations, key frameworks, and fundamental assumptions that underlie the methods I've implemented in CausalPilot.</p>"},{"location":"causal_inference_intro/#1-the-fundamental-problem-of-causal-inference","title":"1. The Fundamental Problem of Causal Inference","text":""},{"location":"causal_inference_intro/#beyond-correlation-the-need-for-causal-thinking","title":"Beyond Correlation: The Need for Causal Thinking","text":"<p>Traditional statistical methods excel at identifying associations and correlations between variables. However, answering questions about the effects of interventions requires a fundamentally different approach. Consider these examples:</p> <ul> <li>Predictive Question: What is the probability a patient will recover given they take medication X?</li> <li>Causal Question: What would happen to the patient's recovery if we intervene and give them medication X?</li> </ul> <p>The key distinction lies in the counterfactual nature of causal questions: I seek to understand what would happen under different interventions, not just what I observe.</p>"},{"location":"causal_inference_intro/#the-missing-data-problem","title":"The Missing Data Problem","text":"<p>The fundamental challenge in causal inference stems from the fact that I can never observe what would have happened to the same unit under a different treatment condition. This is known as the \"fundamental problem of causal inference\".</p> <p>For each individual $i$: - I observe either $Y_i(1)$ (outcome under treatment) OR $Y_i(0)$ (outcome under control) - I never observe both potential outcomes for the same individual - The unobserved outcome represents missing data that cannot be recovered through repeated experiments</p>"},{"location":"causal_inference_intro/#2-frameworks-for-causal-inference","title":"2. Frameworks for Causal Inference","text":""},{"location":"causal_inference_intro/#21-potential-outcomes-framework-rubin-causal-model","title":"2.1 Potential Outcomes Framework (Rubin Causal Model)","text":"<p>The potential outcomes framework provides a mathematical foundation for causal inference by explicitly modeling counterfactual outcomes.</p>"},{"location":"causal_inference_intro/#core-concepts","title":"Core Concepts","text":"<p>Units: The entities (people, places, things) on which treatments operate at specific times.</p> <p>Treatments: Interventions whose effects I wish to assess. For simplicity, consider binary treatment $T \\in {0,1}$.</p> <p>Potential Outcomes: For each unit $i$, I define: - $Y_i(1)$: Outcome if unit $i$ receives treatment ($T=1$) - $Y_i(0)$: Outcome if unit $i$ receives control ($T=0$)</p> <p>Observed Outcome: What I actually observe: $$ Y_i = T_i \\cdot Y_i(1) + (1 - T_i) \\cdot Y_i(0) $$</p> <p>Individual Treatment Effect: The causal effect for unit $i$: $$ \\tau_i = Y_i(1) - Y_i(0) $$</p>"},{"location":"causal_inference_intro/#key-estimands","title":"Key Estimands","text":"<p>Average Treatment Effect (ATE): $$ ATE = E[Y_i(1) - Y_i(0)] = E[Y_i(1)] - E[Y_i(0)] $$</p> <p>Average Treatment Effect on the Treated (ATT): $$ ATT = E[Y_i(1) - Y_i(0) | T_i = 1] $$</p> <p>Conditional Average Treatment Effect (CATE): $$ CATE(x) = E[Y_i(1) - Y_i(0) | X_i = x] $$</p>"},{"location":"causal_inference_intro/#22-structural-causal-models-and-causal-graphs","title":"2.2 Structural Causal Models and Causal Graphs","text":"<p>The graphical approach to causation uses directed acyclic graphs (DAGs) to represent causal relationships and identify conditions under which causal effects can be estimated.</p>"},{"location":"causal_inference_intro/#causal-graphs","title":"Causal Graphs","text":"<p>A causal graph $G = (V, E)$ consists of: - Vertices (V): Variables in the system - Directed Edges (E): Direct causal relationships - DAG Property: No directed cycles (A cannot cause B which causes A)</p> <p>Causal Interpretation: An edge $X \\to Y$ means $X$ is a direct cause of $Y$, holding all other variables constant.</p>"},{"location":"causal_inference_intro/#path-analysis","title":"Path Analysis","text":"<ul> <li>Causal Paths: Sequences of edges following the direction of arrows ($X \\to Z \\to Y$)</li> <li>Backdoor Paths: Paths from treatment to outcome that begin with an arrow into the treatment</li> <li>Confounding Paths: Backdoor paths that create spurious associations</li> </ul>"},{"location":"causal_inference_intro/#3-fundamental-identification-assumptions","title":"3. Fundamental Identification Assumptions","text":"<p>To move from observational data to causal conclusions, I must make untestable assumptions about the data-generating process.</p>"},{"location":"causal_inference_intro/#31-unconfoundedness-ignorability","title":"3.1 Unconfoundedness (Ignorability)","text":"<p>Assumption: Treatment assignment is unconfounded given observed covariates $X$.</p> <p>Mathematical Statement: $$ {Y_i(0), Y_i(1)} \\perp T_i | X_i $$</p> <p>Interpretation: Conditional on $X$, treatment assignment is as good as random\u2014there are no unmeasured confounders affecting both treatment and outcome.</p> <p>Plausibility: This is the most critical and often most questionable assumption. It requires: - All relevant confounders are observed and measured - No hidden bias from unmeasured variables - Complete understanding of the selection mechanism</p>"},{"location":"causal_inference_intro/#32-positivity-overlap","title":"3.2 Positivity (Overlap)","text":"<p>Assumption: Every unit has positive probability of receiving any treatment level.</p> <p>Mathematical Statement: $$ 0 &lt; P(T_i = t | X_i = x) &lt; 1 $$ for all $t \\in {0,1}$ and all $x$ in the support of $X$.</p> <p>Interpretation: There is sufficient overlap between treatment groups across all covariate levels.</p> <p>Violations:  - Structural violations: Some subgroups never receive treatment (e.g., men don't get pregnant) - Random violations: Small sample sizes lead to missing combinations</p>"},{"location":"causal_inference_intro/#33-consistency-sutva","title":"3.3 Consistency (SUTVA)","text":"<p>The Stable Unit Treatment Value Assumption (SUTVA) has two components:</p>"},{"location":"causal_inference_intro/#no-interference","title":"No Interference","text":"<p>Assumption: Treatment of one unit doesn't affect outcomes of other units.</p> <p>Mathematical Statement: $$ Y_i(t_1, ..., t_i, ..., t_n) = Y_i(t_i) $$</p> <p>Violations: Network effects, spillovers, general equilibrium effects</p>"},{"location":"causal_inference_intro/#treatment-variation-irrelevance","title":"Treatment Variation Irrelevance","text":"<p>Assumption: There is only one version of each treatment level.</p> <p>Violations: Multiple versions of \"treatment\" with different effects</p>"},{"location":"causal_inference_intro/#4-identification-strategies","title":"4. Identification Strategies","text":""},{"location":"causal_inference_intro/#41-backdoor-criterion","title":"4.1 Backdoor Criterion","text":"<p>The backdoor criterion provides a graphical method for identifying valid adjustment sets.</p> <p>Definition: A set of variables $Z$ satisfies the backdoor criterion relative to treatment $T$ and outcome $Y$ if: 1. $Z$ blocks all backdoor paths from $T$ to $Y$ 2. $Z$ contains no descendants of $T$</p> <p>Adjustment Formula: If $Z$ satisfies the backdoor criterion: $$ P(Y(t)) = \\sum_z P(Y | T = t, Z = z) P(Z = z) $$</p>"},{"location":"causal_inference_intro/#algorithm-for-finding-adjustment-sets","title":"Algorithm for Finding Adjustment Sets","text":"<ol> <li>Identify all paths from treatment $T$ to outcome $Y$</li> <li>Classify paths as front-door (causal) or backdoor (confounding)</li> <li>Find blocking sets that close all backdoor paths without blocking front-door paths</li> <li>Exclude descendants of the treatment to avoid post-treatment bias</li> </ol>"},{"location":"causal_inference_intro/#42-instrumental-variables","title":"4.2 Instrumental Variables","text":"<p>When unconfoundedness fails, instrumental variables can provide identification under different assumptions.</p> <p>Definition: An instrument $Z$ for the effect of $T$ on $Y$ satisfies: 1. Relevance: $Z$ affects $T$ (correlation condition) 2. Exclusion: $Z$ affects $Y$ only through $T$ (exogeneity condition) 3. Monotonicity: $Z$ affects $T$ in the same direction for all units</p> <p>Two-Stage Least Squares (2SLS): 1. First stage: Predict $T$ using $Z$ 2. Second stage: Use predicted $\\hat{T}$ to estimate effect on $Y$</p>"},{"location":"causal_inference_intro/#5-estimation-approaches","title":"5. Estimation Approaches","text":""},{"location":"causal_inference_intro/#51-regression-based-methods","title":"5.1 Regression-Based Methods","text":"<p>Simple Regression Adjustment: $$ E[Y | T = 1, X] - E[Y | T = 0, X] $$</p> <p>Challenges: - Requires correct specification of $E[Y | T, X]$ - Sensitive to model misspecification - Parametric assumptions may be violated</p>"},{"location":"causal_inference_intro/#52-propensity-score-methods","title":"5.2 Propensity Score Methods","text":"<p>Propensity Score: The probability of treatment given covariates: $$ e(x) = P(T = 1 | X = x) $$</p> <p>Key Property: If unconfoundedness holds given $X$, it also holds given $e(X)$.</p> <p>Estimation Methods: 1. Matching: Match treated and control units with similar propensity scores 2. Stratification: Group units into propensity score strata 3. Weighting: Weight by inverse propensity scores (IPW)</p>"},{"location":"causal_inference_intro/#53-modern-machine-learning-approaches","title":"5.3 Modern Machine Learning Approaches","text":"<p>Double/Debiased Machine Learning: Uses cross-fitting to estimate nuisance functions while maintaining valid inference.</p> <p>Causal Forests: Extends random forests to estimate heterogeneous treatment effects.</p> <p>Meta-learners: Use any supervised learning algorithm to estimate treatment effects.</p>"},{"location":"causal_inference_intro/#6-threats-to-causal-inference","title":"6. Threats to Causal Inference","text":""},{"location":"causal_inference_intro/#61-selection-bias","title":"6.1 Selection Bias","text":"<p>Problem: Systematic differences between treatment groups that affect outcomes.</p> <p>Sources: - Self-selection into treatment - Non-random treatment assignment - Unmeasured confounding</p> <p>Solutions: Randomization, natural experiments, instrumental variables</p>"},{"location":"causal_inference_intro/#62-confounding","title":"6.2 Confounding","text":"<p>Problem: Variables that affect both treatment and outcome, creating spurious associations.</p> <p>Types: - Measured confounding: Can be addressed through adjustment - Unmeasured confounding: Requires additional assumptions or design features</p>"},{"location":"causal_inference_intro/#63-post-treatment-bias","title":"6.3 Post-Treatment Bias","text":"<p>Problem: Conditioning on variables affected by treatment can introduce bias.</p> <p>Example: Studying effect of education on income while controlling for occupation (which education affects).</p> <p>Solution: Only adjust for pre-treatment variables.</p>"},{"location":"causal_inference_intro/#7-sensitivity-analysis","title":"7. Sensitivity Analysis","text":"<p>Given the reliance on untestable assumptions, sensitivity analysis examines how conclusions change under assumption violations.</p>"},{"location":"causal_inference_intro/#71-unmeasured-confounding","title":"7.1 Unmeasured Confounding","text":"<p>Rosenbaum Bounds: Quantify how strong unmeasured confounding would need to be to explain away the estimated effect.</p> <p>Simulation-Based Approaches: Model specific confounding scenarios and assess robustness.</p>"},{"location":"causal_inference_intro/#72-model-uncertainty","title":"7.2 Model Uncertainty","text":"<p>Cross-Validation: Assess stability across different model specifications.</p> <p>Ensemble Methods: Combine multiple estimation approaches.</p>"},{"location":"causal_inference_intro/#8-study-design-considerations","title":"8. Study Design Considerations","text":""},{"location":"causal_inference_intro/#81-randomized-experiments","title":"8.1 Randomized Experiments","text":"<p>Gold Standard: Randomization ensures treatment assignment is independent of potential outcomes.</p> <p>Advantages: - Eliminates selection bias - Balances measured and unmeasured confounders - Provides valid causal inference under minimal assumptions</p> <p>Limitations: - May not be feasible or ethical - External validity concerns - Compliance issues</p>"},{"location":"causal_inference_intro/#82-natural-experiments","title":"8.2 Natural Experiments","text":"<p>Quasi-Randomization: Exploit natural variation in treatment assignment.</p> <p>Examples: - Regression discontinuity (arbitrary cutoffs) - Instrumental variables (plausibly exogenous variation) - Difference-in-differences (before/after, treatment/control)</p>"},{"location":"causal_inference_intro/#83-observational-studies","title":"8.3 Observational Studies","text":"<p>Challenge: Must rely on stronger assumptions about selection mechanism.</p> <p>Best Practices: - Careful measurement of potential confounders - Use of multiple identification strategies - Extensive sensitivity analysis - Theory-guided variable selection</p>"},{"location":"causal_inference_intro/#9-software-implementations","title":"9. Software Implementations","text":"<p>Modern causal inference benefits from specialized software that implements these methods:</p> <ul> <li>DoWhy: Provides a four-step workflow (model, identify, estimate, refute)</li> <li>CausalML: Focuses on machine learning approaches to heterogeneous effects</li> <li>CausalInference: Classical propensity score and matching methods</li> <li>CausalPilot: My unified framework for multiple estimation approaches</li> </ul>"},{"location":"causal_inference_intro/#conclusion","title":"Conclusion","text":"<p>Causal inference requires a careful combination of: 1. Theoretical Understanding: Potential outcomes, graphs, identification assumptions 2. Design Considerations: How to collect data that supports causal conclusions 3. Statistical Methods: Appropriate estimation techniques for the identification strategy 4. Sensitivity Analysis: Assessment of robustness to assumption violations</p> <p>Success in causal inference depends more on credible identification strategies than on sophisticated estimation methods. The most important question is not \"What is the fanciest method?\" but rather \"What assumptions am I making, and how plausible are they?\".</p>"},{"location":"causal_inference_intro/#references","title":"References","text":"<ul> <li>Siebert, J. (2024). \"Causal inference: An introduction.\" Fraunhofer IESE Blog.</li> <li>Pearl, J. (2010). \"An Introduction to Causal Inference.\" Journal of Causal Inference.</li> <li>Pearl, J. (2009). \"Causal inference in statistics: An overview.\" Statistical Science.</li> <li>Rubin, D. B. (2005). \"Causal inference using potential outcomes.\" Journal of the American Statistical Association.</li> <li>Pearl, J. (1995). \"Causal diagrams for empirical research.\" Biometrika.</li> <li>Holland, P. W. (1986). \"Statistics and causal inference.\" Journal of the American Statistical Association.</li> <li>Shpitser, I. et al. \"A complete identification procedure for causal effects.\" Proceedings of UAI.</li> </ul>"},{"location":"estimator_theory/","title":"Theory Behind Causal Inference Estimators","text":""},{"location":"estimator_theory/#introduction","title":"Introduction","text":"<p>This document provides a comprehensive theoretical foundation for the causal inference estimators I have implemented in CausalPilot. Each method addresses specific challenges in causal inference while making different assumptions about the data-generating process and treatment effect heterogeneity.</p>"},{"location":"estimator_theory/#1-doubledebiased-machine-learning-doubleml","title":"1. Double/Debiased Machine Learning (DoubleML)","text":""},{"location":"estimator_theory/#theoretical-foundation","title":"Theoretical Foundation","text":"<p>Double Machine Learning addresses the challenge of estimating causal effects in high-dimensional settings where traditional parametric methods may be biased or inefficient. I chose to implement this because it allows for the use of powerful machine learning models (like Random Forests or Gradient Boosting) to control for confounders without introducing regularization bias into the causal estimate.</p>"},{"location":"estimator_theory/#mathematical-framework","title":"Mathematical Framework","text":"<p>Consider the partially linear regression model: $$ Y = \\theta T + g(X) + U $$ $$ T = m(X) + V $$</p> <p>Where: - $Y$ is the outcome variable - $T$ is the treatment variable - $X$ are confounding covariates - $\\theta$ is the causal parameter of interest - $g(X) = E[Y|X]$ is the conditional expectation function - $m(X) = E[T|X]$ is the propensity score - $U, V$ are error terms with $E[U|X] = E[V|X] = 0$</p>"},{"location":"estimator_theory/#key-theoretical-components","title":"Key Theoretical Components","text":""},{"location":"estimator_theory/#1-neyman-orthogonality","title":"1. Neyman Orthogonality","text":"<p>The estimating equation satisfies: $$ E[\\psi(W; \\theta, \\eta)] = 0 $$ where $\\psi$ is approximately insensitive to perturbations in nuisance parameters $\\eta = (g, m)$. This ensures that small errors in nuisance function estimation don't dramatically affect the causal parameter estimate.</p>"},{"location":"estimator_theory/#2-cross-fitting-procedure","title":"2. Cross-Fitting Procedure","text":"<p>I implemented cross-fitting to avoid overfitting: 1. Sample Splitting: Randomly partition data into $K$ folds 2. Nuisance Estimation: For each fold $k$:    - Train models $\\hat{g}{-k}$ and $\\hat{m}{-k}$ on data excluding fold $k$    - Predict on fold $k$ to get $\\hat{g}{-k}(X_i)$ and $\\hat{m}{-k}(X_i)$ for $i \\in k$ 3. Parameter Estimation: Solve the moment condition using all folds</p>"},{"location":"estimator_theory/#3-moment-condition","title":"3. Moment Condition","text":"<p>The orthogonal moment condition for the partially linear model is: $$ \\psi(W_i; \\theta, \\eta) = (T_i - m(X_i))(Y_i - \\theta T_i - g(X_i)) $$</p>"},{"location":"estimator_theory/#theoretical-guarantees","title":"Theoretical Guarantees","text":"<p>Under regularity conditions, the DoubleML estimator is: - $\\sqrt{n}$-consistent: Converges at the parametric rate - Asymptotically normal: Normal limiting distribution enables confidence intervals - Semiparametrically efficient: Achieves the efficiency bound</p>"},{"location":"estimator_theory/#2-causal-forest","title":"2. Causal Forest","text":""},{"location":"estimator_theory/#theoretical-foundation_1","title":"Theoretical Foundation","text":"<p>Causal Forests extend random forests to estimate heterogeneous treatment effects by building trees that explicitly optimize for treatment effect heterogeneity rather than prediction accuracy. I included this estimator to allow users to understand who benefits most from a treatment.</p>"},{"location":"estimator_theory/#mathematical-framework_1","title":"Mathematical Framework","text":"<p>The goal is to estimate the Conditional Average Treatment Effect (CATE): $$ \\tau(x) = E[Y(1) - Y(0)|X = x] $$</p> <p>Where $Y(1)$ and $Y(0)$ are potential outcomes under treatment and control.</p>"},{"location":"estimator_theory/#algorithm-structure","title":"Algorithm Structure","text":""},{"location":"estimator_theory/#1-honest-tree-construction","title":"1. Honest Tree Construction","text":"<p>Each tree is built using sample splitting: - I: Subsample for determining splits - J: Disjoint subsample for estimating treatment effects</p> <p>This \"honesty\" prevents overfitting and ensures valid statistical inference.</p>"},{"location":"estimator_theory/#2-splitting-criterion","title":"2. Splitting Criterion","text":"<p>Instead of optimizing prediction accuracy, causal trees maximize treatment effect heterogeneity. For a potential split of parent node $P$ into children $C_1$ and $C_2$:</p> <p>$$ \\text{Split criterion} = \\frac{n_1 n_2}{n_1 + n_2} \\times (\\hat{\\tau}(C_1) - \\hat{\\tau}(C_2))^2 $$</p> <p>Where $\\hat{\\tau}(C_k)$ is the estimated treatment effect in child $k$.</p>"},{"location":"estimator_theory/#3-treatment-effect-estimation","title":"3. Treatment Effect Estimation","text":"<p>In each leaf, the treatment effect is estimated using: $$ \\hat{\\tau}{leaf} = \\frac{\\sum{i \\in leaf} T_i Y_i}{\\sum_{i \\in leaf} T_i} - \\frac{\\sum_{i \\in leaf} (1-T_i) Y_i}{\\sum_{i \\in leaf} (1-T_i)} $$</p>"},{"location":"estimator_theory/#4-forest-aggregation","title":"4. Forest Aggregation","text":"<p>The final estimate is a weighted average: $$ \\hat{\\tau}(x) = \\sum_{i=1}^n \\alpha_i(x) Y_i $$</p> <p>Where $\\alpha_i(x)$ represents the weight given to observation $i$ when predicting at point $x$, determined by how often $i$ and $x$ fall in the same leaf across trees.</p>"},{"location":"estimator_theory/#3-meta-learners-t-learner-s-learner-x-learner","title":"3. Meta-Learners (T-learner, S-learner, X-learner)","text":""},{"location":"estimator_theory/#theoretical-framework","title":"Theoretical Framework","text":"<p>Meta-learners are flexible approaches that use any supervised learning algorithm as a \"base learner\" to estimate treatment effects.</p>"},{"location":"estimator_theory/#t-learner-two-model-approach","title":"T-learner (Two-Model Approach)","text":""},{"location":"estimator_theory/#algorithm","title":"Algorithm","text":"<ol> <li>Separate Models: Train two models:</li> <li>$\\mu_0(x) = E[Y|X = x, T = 0]$ (control model)</li> <li> <p>$\\mu_1(x) = E[Y|X = x, T = 1]$ (treatment model)</p> </li> <li> <p>Effect Estimation:     $$ \\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x) $$</p> </li> </ol>"},{"location":"estimator_theory/#advantages-and-limitations","title":"Advantages and Limitations","text":"<p>Advantages: - Simple and intuitive - Flexible choice of base learner</p> <p>Limitations: - Requires sufficient data in both treatment groups - No regularization across treatment groups</p>"},{"location":"estimator_theory/#s-learner-single-model-approach","title":"S-learner (Single-Model Approach)","text":""},{"location":"estimator_theory/#algorithm_1","title":"Algorithm","text":"<ol> <li> <p>Combined Model: Train single model:    $$ \\mu(x,t) = E[Y|X = x, T = t] $$</p> </li> <li> <p>Effect Estimation:    $$ \\hat{\\tau}(x) = \\hat{\\mu}(x,1) - \\hat{\\mu}(x,0) $$</p> </li> </ol>"},{"location":"estimator_theory/#advantages-and-limitations_1","title":"Advantages and Limitations","text":"<p>Advantages: - Uses all data for model training - Good when treatment effects are small</p> <p>Limitations: - May miss treatment effect heterogeneity if base learner doesn't capture interactions well</p>"},{"location":"estimator_theory/#x-learner-cross-learner","title":"X-learner (Cross-learner)","text":"<p>I implemented the X-Learner specifically for cases where one treatment group is much larger than the other (e.g., a small treatment group and a large control group).</p>"},{"location":"estimator_theory/#algorithm_2","title":"Algorithm","text":"<ol> <li>Stage 1: Train separate outcome models (like T-learner)</li> <li>Stage 2: Estimate individual treatment effects:</li> <li>For treated units: $\\hat{\\tau}_{1i} = Y_i - \\hat{\\mu}_0(X_i)$</li> <li>For control units: $\\hat{\\tau}_{0i} = \\hat{\\mu}_1(X_i) - Y_i$</li> <li>Stage 3: Train models to predict these effects:</li> <li>$\\hat{\\tau}_1(x) = E[\\hat{\\tau}_1|X = x, T = 1]$  </li> <li>$\\hat{\\tau}_0(x) = E[\\hat{\\tau}_0|X = x, T = 0]$</li> <li>Final Estimate: Weighted combination using propensity scores</li> </ol>"},{"location":"estimator_theory/#theoretical-advantages","title":"Theoretical Advantages","text":"<ul> <li>Doubly Robust: Good performance if either outcome models or propensity model is correct</li> <li>Efficiency: Often more efficient than T-learner or S-learner</li> </ul>"},{"location":"estimator_theory/#4-instrumental-variables-iv2sls","title":"4. Instrumental Variables (IV2SLS)","text":""},{"location":"estimator_theory/#theoretical-foundation_2","title":"Theoretical Foundation","text":"<p>Sometimes, we have unobserved confounders ($U$) that affect both $T$ and $Y$. In this case, adjusting for $X$ is not enough. I implemented Two-Stage Least Squares (IV2SLS) to handle this scenario.</p>"},{"location":"estimator_theory/#mathematical-framework_2","title":"Mathematical Framework","text":"<p>We assume the following structural equations: $$ Y = \\beta T + \\gamma X + U $$ $$ T = \\delta Z + \\phi X + V $$</p> <p>Where $Z$ is an Instrument that satisfies: 1. Relevance: $Z$ affects $T$ ($\\delta \\neq 0$). 2. Exclusion: $Z$ affects $Y$ only through $T$ (no direct path $Z \\to Y$). 3. Independence: $Z$ is independent of $U$.</p>"},{"location":"estimator_theory/#algorithm_3","title":"Algorithm","text":"<ol> <li>First Stage: Regress $T$ on $Z$ and $X$ to get $\\hat{T}$.    $$ \\hat{T} = \\hat{\\delta} Z + \\hat{\\phi} X $$</li> <li>Second Stage: Regress $Y$ on $\\hat{T}$ and $X$.    $$ Y = \\beta_{IV} \\hat{T} + \\gamma_{IV} X + \\epsilon $$</li> </ol> <p>This isolates the variation in $T$ that is caused by $Z$ (which is clean) and ignores the variation caused by $U$ (which is confounded).</p>"},{"location":"estimator_theory/#5-comparison-of-methods","title":"5. Comparison of Methods","text":"Method Best Use Case Key Assumption Computational Cost DoubleML High-dimensional confounders Unconfoundedness Medium Causal Forest Effect heterogeneity discovery Unconfoundedness High T-learner Clear treatment groups Sufficient sample in each group Low S-learner Small treatment effects Treatment-outcome relationship captured Low X-learner Imbalanced treatment groups Either outcomes or propensity correct Medium IV2SLS Unobserved confounding Valid Instrument Low"},{"location":"estimator_theory/#conclusion","title":"Conclusion","text":"<p>Each estimator offers different strengths for various causal inference scenarios. DoubleML excels with high-dimensional data, Causal Forests discover complex heterogeneity patterns, and meta-learners provide flexible, interpretable approaches. The choice depends on the specific research question, data characteristics, and computational constraints.</p>"},{"location":"estimator_theory/#references","title":"References","text":"<ul> <li>Chernozhukov et al. (2018). \"Double/debiased machine learning for treatment and structural parameters.\"</li> <li>Wager, S., &amp; Athey, S. (2018). \"Estimation and inference of heterogeneous treatment effects using random forests.\"</li> <li>K\u00fcnzel, S. R., et al. (2019). \"Metalearners for estimating heterogeneous treatment effects using machine learning.\"</li> <li>Angrist, J. D., &amp; Pischke, J. S. (2009). \"Mostly harmless econometrics: An empiricist's companion.\"</li> </ul>"}]}